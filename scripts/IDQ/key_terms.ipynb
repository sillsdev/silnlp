{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much time was spent on this, but the original idea was to find and correct instances of proper nouns in drafts using the key terms lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from silnlp.common.corpus import load_corpus\n",
    "\n",
    "# paths for generic Major KT lists\n",
    "metadata_path = Path(\"silnlp/assets/Major-metadata.txt\")\n",
    "vrefs_path = Path(\"silnlp/assets/Major-vrefs.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create mappings of proper noun KTs to their instances and verse refs to the proper nouns they contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_gloss_path = Path(\"silnlp/assets/fr-Major-glosses.txt\") # en  fr\n",
    "trg_gloss_path = Path(\"test_S/MT/terms/bcw-bcw_2024_02_21-Major-renderings.txt\") # lmp-lmp_2024_02_16  bcw-bcw_2024_02_21\n",
    "pair = \"fr_bcw\" # en_lmp  fr_bcw\n",
    "\n",
    "proper_nouns = defaultdict(dict)\n",
    "for i, (meta, vref, src_gloss, trg_gloss) in enumerate(zip(load_corpus(metadata_path), load_corpus(vrefs_path), load_corpus(src_gloss_path), load_corpus(trg_gloss_path))):\n",
    "    term, pt_cat, sem_cat = meta.split(\"\\t\") # orig lang term, Paratext category (PN, FL, RE, etc.), semantic category (person, grasses, containers, etc.)\n",
    "    instances = vref.split(\"\\t\") # all occurrences of the term\n",
    "    src_glosses = src_gloss.split(\"\\t\") # all potential glosses for term\n",
    "    trg_glosses = trg_gloss.split(\"\\t\")\n",
    "\n",
    "    if pt_cat == \"PN\" and trg_glosses != [\"\"]:\n",
    "        proper_nouns[i][\"glosses\"] = (src_glosses, trg_glosses)\n",
    "        proper_nouns[i][\"instances\"] = instances # might want to give this further structure, i.e. be a dict w/ book:chapter:[instances]\n",
    "\n",
    "with open(\"KT_to_vrefs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(proper_nouns, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Create verse-to-KTs dict\n",
    "vref_to_KTs = defaultdict(list)\n",
    "for i, pn_dict in proper_nouns.items():\n",
    "    for vref in pn_dict[\"instances\"]:\n",
    "        vref_to_KTs[vref].append(i)\n",
    "with open(\"vref_to_KTs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vref_to_KTs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to correct the translations of known instances of KTs\n",
    "\n",
    "(This is very preliminary work so I have no idea if it's useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from silnlp.common.corpus import load_corpus\n",
    "from pathlib import Path\n",
    "from machine.corpora import ScriptureRef\n",
    "from silnlp.alignment.utils import compute_alignment_scores\n",
    "\n",
    "book_name = \"08RUT\"\n",
    "vrefs = [ScriptureRef.parse(ref) for ref in load_corpus(Path(f\"{book_name}_vrefs.txt\"))]\n",
    "src_path = Path(f\"{book_name}_src_sents.txt\")\n",
    "trg_path = Path(f\"{book_name}_trg_sents.txt\")\n",
    "\n",
    "# always uses LatinWordTokenizer\n",
    "sym_align_path = Path(f\"{book_name}_sym-align.txt\")\n",
    "scores = compute_alignment_scores(src_path, trg_path, aligner_id=\"eflomal\", sym_align_path=sym_align_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.tokenization import LatinWordTokenizer\n",
    "from machine.corpora import TextFileTextCorpus\n",
    "from machine.scripture import VerseRef\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "src_lines = [line.segment for line in TextFileTextCorpus(src_path).tokenize(LatinWordTokenizer()).lowercase()]\n",
    "trg_lines = [line.segment for line in TextFileTextCorpus(trg_path).tokenize(LatinWordTokenizer())]\n",
    "src_lines_raw = load_corpus(src_path)\n",
    "trg_lines_raw = load_corpus(trg_path)\n",
    "\n",
    "align_lines = [[(lambda x: (int(x[0]), int(x[1])))(pair.split(\":\")[0].split(\"-\")) for pair in line.split()] for line in load_corpus(sym_align_path)]\n",
    "\n",
    "book = \"RUT\"\n",
    "with open(\"vref_to_KTs.json\", encoding=\"utf-8\") as f:\n",
    "    vref_to_KTs = json.load(f)\n",
    "with open(\"KT_to_vrefs.json\", encoding=\"utf-8\") as f:\n",
    "    KT_to_vrefs = json.load(f)\n",
    "\n",
    "term_ids = set()\n",
    "exp_vrefs = set()\n",
    "for ref, ids in vref_to_KTs.items():\n",
    "    if VerseRef.from_string(ref).book == book:\n",
    "        term_ids.update(ids)\n",
    "        exp_vrefs.add(ref)\n",
    "src_terms = set()\n",
    "trg_terms = set()\n",
    "for id in term_ids:\n",
    "    src_terms.update(KT_to_vrefs[str(id)][\"glosses\"][0])\n",
    "    trg_terms.update(KT_to_vrefs[str(id)][\"glosses\"][1])\n",
    "print(src_terms)\n",
    "\n",
    "for ref,src_line,trg_line,align_pairs,trg_line_raw in zip(vrefs, src_lines, trg_lines, align_lines,trg_lines_raw):\n",
    "    if str(ref.verse_ref) not in vref_to_KTs.keys():\n",
    "        continue\n",
    "    if ref.verse_num == 0 or ref.path[0].name != \"\": # the ScriptureRefs I'm testing with have an empty ScriptureElement in the path so is_verse doesn't work\n",
    "        continue\n",
    "\n",
    "    found = []\n",
    "    for term_id in vref_to_KTs[str(ref.verse_ref)]:\n",
    "        glosses = [gloss.lower() for gloss in KT_to_vrefs[str(term_id)][\"glosses\"][0]]\n",
    "        min_dist = (0, 0, 100) # gloss idx of closest match, tok idx of closest match, distance\n",
    "        for i, gloss in enumerate(glosses): # could adjust this to look at n-grams, where n is the number of words in the gloss\n",
    "            for j, tok in enumerate(src_line):\n",
    "                if (j, term_id) in found:\n",
    "                    continue\n",
    "                dist = nltk.edit_distance(gloss, tok) / len(tok)\n",
    "                if dist < min_dist[2]:\n",
    "                    min_dist = (i, j, dist)\n",
    "        if min_dist[2] < .3:\n",
    "            found.append((min_dist[1], term_id))\n",
    "\n",
    "    # replace word(s) in target text\n",
    "    for src_idx, term_id in found:\n",
    "        trg_idxs = [pair[1] for pair in align_pairs if pair[0] == src_idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
