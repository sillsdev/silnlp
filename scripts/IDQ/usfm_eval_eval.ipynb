{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USFM marker placement metric evaluation\n",
    "\n",
    "This code and the corresponding constructed gold data can be used to recreate the results of the evaluation of various metrics for evaluating USFM marker placement.\n",
    "\n",
    "I used a copy of the jaro similarity code rather than the library to get access to the number of \"half transposes\" to evaluate it as a potential metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "from scipy.stats import pearsonr\n",
    "# import jaro\n",
    "\n",
    "from machine.corpora import UsfmFileText\n",
    "from machine.tokenization import WhitespaceTokenizer\n",
    "\n",
    "class WhitespaceMarkerTokenizer(WhitespaceTokenizer):\n",
    "    def _is_whitespace(self, c: str) -> bool:\n",
    "        return super()._is_whitespace(c) or c == \"\\\\\" or c == \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Code from https://github.com/richmilne/JaroWinkler/blob/master/jaro/jaro.py'''\n",
    "\n",
    "def jaro(s1, s2):\n",
    "    # s1 is always the shorter string\n",
    "    if len(s2) < len(s1):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    if len(s1) == 0:\n",
    "        print(\"empty sequence\")\n",
    "        print(s1)\n",
    "        print(s2)\n",
    "        return 0, 0\n",
    "\n",
    "    search_range = max((len(s2) // 2) - 1, 0)\n",
    "    matched1 = [0] * len(s1)\n",
    "    matched2 = [0] * len(s2)\n",
    "    num_matches = 0\n",
    "\n",
    "    for i, char in enumerate(s1):\n",
    "        for j in range(max(i - search_range, 0), min(i + search_range, len(s2) - 1) + 1):\n",
    "            if not matched2[j] and char == s2[j]:\n",
    "                matched1[i] = matched2[j] = 1\n",
    "                num_matches += 1\n",
    "                break\n",
    "    \n",
    "    if num_matches == 0:\n",
    "        print(\"no matches\")\n",
    "        print(s1)\n",
    "        print(s2)\n",
    "        return 0, 0\n",
    "\n",
    "    # number of matched tokens in s1 such that if it is the ith matched token, the the ith matched token in s2 (in linear order) is not what it was matched with\n",
    "    # this number divided by 2 is the number of transpositions\n",
    "    half_transposes = 0\n",
    "    j = 0\n",
    "    for i, matched in enumerate(matched1):\n",
    "        if not matched:\n",
    "            continue\n",
    "        while not matched2[j]:\n",
    "            j += 1\n",
    "        if s1[i] != s2[j]:\n",
    "            half_transposes += 1\n",
    "        j += 1\n",
    "\n",
    "    dist = (\n",
    "        num_matches / len(s1)\n",
    "        + num_matches / len(s2)\n",
    "        + (num_matches - half_transposes // 2) / num_matches\n",
    "        ) / 3\n",
    "    return dist, half_transposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Levenshtein distance\n",
    "raw score: num of insertions/substitutions/deletions/transpositions\n",
    "'''\n",
    "def levenshtein_metrics(gs_toks, ps_toks, num_markers):\n",
    "    dists = [] # edit distance\n",
    "    for gs, ps in zip(gs_toks, ps_toks):\n",
    "        dist = nltk.edit_distance(gs, ps, transpositions=True)\n",
    "        dists.append(dist)\n",
    "\n",
    "    dists_per_10_tokens = [d * 10 / len(gs) for d,gs in zip(dists, gs_toks)]\n",
    "    dists_per_marker = [d / n for d,n in zip(dists, num_markers)]\n",
    "    return dists, dists_per_10_tokens, dists_per_marker\n",
    "\n",
    "'''\n",
    "Jaro similarity\n",
    "raw score:\n",
    "avg of:\n",
    "1. % matches in s1 (m / len(s1))\n",
    "2. % matches in s2 (m / len(s2))\n",
    "3. % matches not transposed ((m-t)/m), t = # \"half transposes\" / 2\n",
    "'''\n",
    "def jaro_metrics(gs_toks, ps_toks, num_markers):\n",
    "    jaro_scores = []\n",
    "    half_transposes = []\n",
    "    for gs, ps in zip(gs_toks, ps_toks):\n",
    "        score, hts = jaro(gs, ps)\n",
    "        jaro_scores.append(score)\n",
    "        half_transposes.append(hts)\n",
    "\n",
    "    ht_per_tok = [t * 10 / len(g) for t,g in zip(half_transposes, gs_toks)]\n",
    "    ht_per_mark = [t / n for t,n in zip(half_transposes, num_markers)]\n",
    "    return jaro_scores, half_transposes, ht_per_tok, ht_per_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prefix = \"41MAT_1_eng\" # 41MAT_1_eng 23ISA_1_spa 43LUK_3_aps 04NUM_1_npi\n",
    "n = 5\n",
    "human_eval_scores = []\n",
    "\n",
    "# custom tokenizer to handle markers\n",
    "tokenizer = WhitespaceMarkerTokenizer()\n",
    "\n",
    "gold_file_path = Path(f\"{test_prefix}/{test_prefix}_gold.SFM\")\n",
    "gold_file_text = UsfmFileText(\"usfm.sty\", \"utf-8-sig\", test_prefix[2:5], gold_file_path, include_markers=True, include_all_text=True)\n",
    "\n",
    "pred_file_paths = [Path(f\"{test_prefix}/{test_prefix}_pred_{i+1}.SFM\") for i in range(n)]\n",
    "\n",
    "sent_idxs = []\n",
    "vrefs = []\n",
    "markers = []\n",
    "gold_sent_toks = []\n",
    "for i, gs in enumerate(gold_file_text):\n",
    "    if gs.text.count(\"\\\\\") == 0:\n",
    "        continue\n",
    "\n",
    "    sent_idxs.append(i)\n",
    "    vrefs.append(gs.ref)\n",
    "    markers.append(re.findall(r\"(?<=\\\\)[^\\s\\\\\\*]*\", gs.text))\n",
    "    \n",
    "    if tokenizer:\n",
    "        gold_sent_toks.append(list(tokenizer.tokenize(gs.text)))\n",
    "    else:\n",
    "        gold_sent_toks.append(gs.text)\n",
    "\n",
    "num_markers = [len(m) for m in markers]\n",
    "\n",
    "'''get scores'''\n",
    "edit_dists, edit_dists_per_10_tokens, edit_dists_per_marker = [], [], []\n",
    "jaro_scores, half_transposes, hts_per_10_tokens, hts_per_marker = [], [], [], []\n",
    "scaled_jaro = []\n",
    "for pred_file_path in pred_file_paths:\n",
    "    pred_file_text = UsfmFileText(\"usfm.sty\", \"utf-8-sig\", test_prefix[2:5], pred_file_path, include_markers=True, include_all_text=True)\n",
    "    pred_sent_toks = []\n",
    "    for i, ps in enumerate(pred_file_text):\n",
    "        if i in sent_idxs:\n",
    "            # add in any markers that didn't get read in (from being at the end of the verse, only happens to paragraph markers)\n",
    "            gold_markers = markers[sent_idxs.index(i)].copy()\n",
    "            pred_markers = re.findall(r\"(?<=\\\\)[^\\s\\\\\\*]*\", ps.text)\n",
    "            for marker in pred_markers:\n",
    "                gold_markers.pop(gold_markers.index(marker))\n",
    "\n",
    "            text = ps.text\n",
    "            for m in gold_markers:\n",
    "                text += f\" \\\\{m}\"\n",
    "\n",
    "            if tokenizer:\n",
    "                pred_sent_toks.append(list(tokenizer.tokenize(text)))\n",
    "            else:\n",
    "                pred_sent_toks.append(text)\n",
    "\n",
    "    dists, dists_per_10_tokens, dists_per_marker = levenshtein_metrics(gold_sent_toks, pred_sent_toks, num_markers)\n",
    "    edit_dists.append(dists)\n",
    "    edit_dists_per_10_tokens.append(dists_per_10_tokens)\n",
    "    edit_dists_per_marker.append(dists_per_marker)\n",
    "\n",
    "    scores, hts, ht_per_tok, ht_per_mark = jaro_metrics(gold_sent_toks, pred_sent_toks, num_markers)\n",
    "    jaro_scores.append(scores)\n",
    "    half_transposes.append(hts)\n",
    "    hts_per_10_tokens.append(ht_per_tok)\n",
    "    hts_per_marker.append(ht_per_mark)\n",
    "\n",
    "\n",
    "'''calculate summary stats'''\n",
    "avg_dist = [sum(ed) / len(vrefs) for ed in edit_dists]\n",
    "avg_dist_per_10_tokens = [sum(edpt) / len(vrefs) for edpt in edit_dists_per_10_tokens]\n",
    "overall_avg_dist_per_10_tokens = [sum(ed) * 10 / sum([len(gs) for gs in gold_sent_toks]) for ed in edit_dists]\n",
    "avg_dist_per_marker = [sum(edpm) / len(vrefs) for edpm in edit_dists_per_marker]\n",
    "overall_avg_dist_per_marker = [sum(ed) / sum(num_markers) for ed in edit_dists]\n",
    "\n",
    "avg_jaro = [sum(js) / len(vrefs) for js in jaro_scores]\n",
    "avg_transposes = [sum(hts) / len(vrefs) for hts in half_transposes]\n",
    "avg_ts_per_10_tokens = [sum(htpt) / len(vrefs) for htpt in hts_per_10_tokens]\n",
    "overall_avg_ts_per_10_tokens = [sum(hts) * 10 / sum([len(g) for g in gold_sent_toks]) for hts in half_transposes]\n",
    "avg_ts_per_marker = [sum(htpm) / len(vrefs) for htpm in hts_per_marker]\n",
    "overall_avg_ts_per_marker = [sum(hts) / sum(num_markers) for hts in half_transposes]\n",
    "\n",
    "'''calculate Pearson's correlation coefficient'''\n",
    "res = pearsonr(human_eval_scores, avg_dist)\n",
    "avg_dist_pearson = res.statistic\n",
    "avg_dist_p = res.pvalue\n",
    "\n",
    "res = pearsonr(human_eval_scores, avg_dist_per_10_tokens)\n",
    "avg_dist_per_10_tokens_pearson = res.statistic\n",
    "avg_dist_per_10_tokens_p = res.pvalue\n",
    "\n",
    "res = pearsonr(human_eval_scores, overall_avg_dist_per_10_tokens)\n",
    "overall_avg_dist_per_10_tokens_pearson = res.statistic\n",
    "overall_avg_dist_per_10_tokens_p = res.pvalue\n",
    "\n",
    "res = pearsonr(human_eval_scores, avg_dist_per_marker)\n",
    "avg_dist_per_marker_pearson = res.statistic\n",
    "avg_dist_per_marker_p = res.pvalue\n",
    "\n",
    "res = pearsonr(human_eval_scores, overall_avg_dist_per_marker)\n",
    "overall_avg_dist_per_marker_pearson = res.statistic\n",
    "overall_avg_dist_per_marker_p = res.pvalue\n",
    "\n",
    "res = pearsonr(human_eval_scores, avg_jaro)\n",
    "avg_jaro_pearson = res.statistic\n",
    "avg_jaro_p = res.pvalue\n",
    "\n",
    "res = pearsonr(human_eval_scores, avg_transposes)\n",
    "avg_transposes_pearson = res.statistic\n",
    "avg_transposes_p = res.pvalue\n",
    "\n",
    "res = pearsonr(human_eval_scores, avg_ts_per_10_tokens)\n",
    "avg_ts_per_10_tokens_pearson = res.statistic\n",
    "avg_ts_per_10_tokens_p = res.pvalue\n",
    "\n",
    "res = pearsonr(human_eval_scores, overall_avg_ts_per_10_tokens)\n",
    "overall_avg_ts_per_10_tokens_pearson = res.statistic\n",
    "overall_avg_ts_per_10_tokens_p = res.pvalue\n",
    "\n",
    "res = pearsonr(human_eval_scores, avg_ts_per_marker)\n",
    "avg_ts_per_marker_pearson = res.statistic\n",
    "avg_ts_per_marker_p = res.pvalue\n",
    "\n",
    "res = pearsonr(human_eval_scores, overall_avg_ts_per_marker)\n",
    "overall_avg_ts_per_marker_pearson = res.statistic\n",
    "overall_avg_ts_per_marker_p = res.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of markers\", num_markers)\n",
    "print(\"sent lengths\", [len(gs) for gs in gold_sent_toks])\n",
    "print(\"avg markers\", sum(num_markers) / len(vrefs))\n",
    "print(\"avg sent len\", sum([len(gs) for gs in gold_sent_toks]) / len(vrefs))\n",
    "print(\"all markers\", markers)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Edit distance\")\n",
    "print(\"edit distance\")\n",
    "for ed in edit_dists:\n",
    "    print(ed)\n",
    "print(\"dist per 10 tokens\")\n",
    "for edpt in edit_dists_per_10_tokens:\n",
    "    print(edpt)\n",
    "print(\"dist per marker\")\n",
    "for edpm in edit_dists_per_marker:\n",
    "    print(edpm)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Jaro similarity\")\n",
    "print(\"jaro\")\n",
    "for js in jaro_scores:\n",
    "    print(js)\n",
    "print(\"scaled jaro\")\n",
    "for sjs in scaled_jaro:\n",
    "    print(sjs)\n",
    "print(\"half transposes\")\n",
    "for hts in half_transposes:\n",
    "    print(hts)\n",
    "print(\"transposes per 10 tokens\")\n",
    "for htpt in hts_per_10_tokens:\n",
    "    print(htpt)\n",
    "print(\"transposes per marker\")\n",
    "for htpm in hts_per_marker:\n",
    "    print(htpm)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"avg dist \\\n",
    "      avg dist per 10 tokens \\\n",
    "      overall avg dist per 10 tokens \\\n",
    "      avg dist per marker \\\n",
    "      overall avg dist per marker\")\n",
    "for i in range(len(pred_file_paths)):\n",
    "    print(f\"{avg_dist[i]}\\t \\\n",
    "        {avg_dist_per_10_tokens[i]}\\t \\\n",
    "        {overall_avg_dist_per_10_tokens[i]}\\t \\\n",
    "        {avg_dist_per_marker[i]}\\t \\\n",
    "        {overall_avg_dist_per_marker[i]}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"avg jaro \\\n",
    "      avg scaled jaro \\\n",
    "      avg transposes \\\n",
    "      avg t's per 10 tokens \\\n",
    "      overall avg t's per 10 tokens \\\n",
    "      avg t's per marker \\\n",
    "      overall avg t's per marker\")\n",
    "for i in range(len(pred_file_paths)):\n",
    "    print(f\"{avg_jaro[i]}\\t \\\n",
    "        {avg_transposes[i]}\\t \\\n",
    "        {avg_ts_per_10_tokens[i]}\\t \\\n",
    "        {overall_avg_ts_per_10_tokens[i]}\\t \\\n",
    "        {avg_ts_per_marker[i]}\\t \\\n",
    "        {overall_avg_ts_per_marker[i]}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"summary stats\") # {avg_scaled_jaro[i]}\\t \\\n",
    "for i in range(len(pred_file_paths)):\n",
    "    print(f\"{avg_dist[i]}\\t \\\n",
    "        {avg_dist_per_10_tokens[i]}\\t \\\n",
    "        {overall_avg_dist_per_10_tokens[i]}\\t \\\n",
    "        {avg_dist_per_marker[i]}\\t \\\n",
    "        {overall_avg_dist_per_marker[i]}\\t \\\n",
    "        {avg_jaro[i]}\\t \\\n",
    "        {avg_transposes[i]}\\t \\\n",
    "        {avg_ts_per_10_tokens[i]}\\t \\\n",
    "        {overall_avg_ts_per_10_tokens[i]}\\t \\\n",
    "        {avg_ts_per_marker[i]}\\t \\\n",
    "        {overall_avg_ts_per_marker[i]}\")\n",
    "\n",
    "print(\"Pearson correlation coefficients and p-values\") # {avg_scaled_jaro_pearson}\\t \\ # {avg_scaled_jaro_p}\\t \\\n",
    "print(f\"{avg_dist_pearson}\\t \\\n",
    "      {avg_dist_per_10_tokens_pearson}\\t \\\n",
    "      {overall_avg_dist_per_10_tokens_pearson}\\t \\\n",
    "      {avg_dist_per_marker_pearson}\\t \\\n",
    "      {overall_avg_dist_per_marker_pearson}\\t \\\n",
    "      {avg_jaro_pearson}\\t \\\n",
    "      {avg_transposes_pearson}\\t \\\n",
    "      {avg_ts_per_10_tokens_pearson}\\t \\\n",
    "      {overall_avg_ts_per_10_tokens_pearson}\\t \\\n",
    "      {avg_ts_per_marker_pearson}\\t \\\n",
    "      {overall_avg_ts_per_marker_pearson}\")\n",
    "print(f\"{avg_dist_p}\\t \\\n",
    "      {avg_dist_per_10_tokens_p}\\t \\\n",
    "      {overall_avg_dist_per_10_tokens_p}\\t \\\n",
    "      {avg_dist_per_marker_p}\\t \\\n",
    "      {overall_avg_dist_per_marker_p}\\t \\\n",
    "      {avg_jaro_p}\\t \\\n",
    "      {avg_transposes_p}\\t \\\n",
    "      {avg_ts_per_10_tokens_p}\\t \\\n",
    "      {overall_avg_ts_per_10_tokens_p}\\t \\\n",
    "      {avg_ts_per_marker_p}\\t \\\n",
    "      {overall_avg_ts_per_marker_p}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
