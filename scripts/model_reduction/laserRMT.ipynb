{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from random import shuffle\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "# from lib.utils import gptq_data_utils\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, M2M100ForConditionalGeneration, PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095841c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelModifier:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.original_weights = {}\n",
    "        self.modified_layers = set()\n",
    "        self.failed_attempts = set()\n",
    "        self.layer_snr = {}\n",
    "\n",
    "    def calculate_snr_for_layer(self, name):\n",
    "        module = self.model.get_submodule(name)\n",
    "        weights = module.weight.double()\n",
    "        S = torch.linalg.svdvals(weights)\n",
    "        max_singular_value = S[0].item()  # First singularity value\n",
    "        weights = weights.detach().cpu()\n",
    "        S = S.detach().cpu()\n",
    "        sigma_estimated = self.estimate_sigma_with_full_iqr(S)\n",
    "        n, m = weights.shape\n",
    "        mp_threshold = self.marchenko_pastur_threshold(sigma_estimated, n, m)\n",
    "\n",
    "        signal = S[S > mp_threshold].sum()\n",
    "        noise = S[S <= mp_threshold].sum()\n",
    "        snr = signal / noise if noise != 0 else float(\"inf\")\n",
    "        snr_ratio = snr / max_singular_value  # Calculates the ratio of SNR to the highest singularity value\n",
    "        del S, weights\n",
    "        torch.cuda.empty_cache()  # Clear PyTorch's CUDA memory cache\n",
    "        gc.collect()\n",
    "        return snr_ratio  # Returns the ratio\n",
    "\n",
    "    def assess_layers_snr(self, layer_types, layer_numbers):\n",
    "        self.layer_snr = {layer_type: [] for layer_type in layer_types}\n",
    "\n",
    "        for name, _ in self.model.named_modules():\n",
    "            for layer_number in layer_numbers:\n",
    "                for layer_type in layer_types:\n",
    "                    if layer_type in name and str(layer_number) in name:\n",
    "                        print(\"*\" * 50, flush=True)\n",
    "                        print(f\"Calculating Signal to Noise Ratio at layer {name}\", flush=True)\n",
    "                        snr_ratio = self.calculate_snr_for_layer(name)\n",
    "                        self.layer_snr[layer_type].append(\n",
    "                            (str(name), snr_ratio.item() if isinstance(snr_ratio, torch.Tensor) else snr_ratio)\n",
    "                        )\n",
    "                        print(f\"Signal to Noise Ratio at layer {name} = {snr_ratio}\", flush=True)\n",
    "                        print(\"*\" * 50, flush=True)\n",
    "\n",
    "    def update_model_reduce_layer(self, layer_type, layer_number):\n",
    "        layer_id = f\"{layer_type}_{layer_number}\"\n",
    "        if layer_id in self.modified_layers:\n",
    "            print(f\"Layer {layer_id} has already been modified. Skipping.\")\n",
    "            return False\n",
    "\n",
    "        for name, module in self.model.named_modules():\n",
    "            if layer_type in name and str(layer_number) in name:\n",
    "                print(f\"Reconstructing layer: {name}\")\n",
    "                original_dtype = module.weight.dtype\n",
    "                self.original_weights[name] = module.weight.detach().clone()\n",
    "                weights = module.weight.double()\n",
    "                U, S, V = torch.linalg.svd(weights, full_matrices=False)\n",
    "\n",
    "                # Estimate sigma using the full IQR method\n",
    "                sigma_estimated_full_iqr = self.estimate_sigma_with_full_iqr(S)\n",
    "\n",
    "                # Calculate Marchenko-Pastur threshold\n",
    "                n, m = weights.shape\n",
    "                mp_threshold_full_iqr = self.marchenko_pastur_threshold(sigma_estimated_full_iqr, n, m)\n",
    "\n",
    "                # Retain only the singular values above the MP threshold\n",
    "                S_reduced = torch.zeros_like(S)\n",
    "                k = (S > mp_threshold_full_iqr).sum().item()\n",
    "                S_reduced[:k] = S[:k]\n",
    "                print(f\"Reduced from {S.shape} to {k}\")\n",
    "\n",
    "                # Reconstruct the matrix using the thresholded singular values\n",
    "                reconstructed_weights = U @ torch.diag(S_reduced) @ V\n",
    "                reconstructed_weights = reconstructed_weights.to(original_dtype)\n",
    "                module.weight = torch.nn.Parameter(reconstructed_weights)\n",
    "                self.modified_layers.add(layer_id)\n",
    "                return True\n",
    "\n",
    "    @staticmethod\n",
    "    def marchenko_pastur_threshold(sigma, n, m):\n",
    "        beta = n / m if n < m else m / n\n",
    "        threshold = sigma * np.sqrt((1 + np.sqrt(beta)) ** 2)\n",
    "        return threshold\n",
    "\n",
    "    # Calculate an estimate of the standard deviation of the singular values based on Inter Quantile Range\n",
    "    @staticmethod\n",
    "    def estimate_sigma_with_full_iqr(S):\n",
    "        q75 = torch.quantile(S, 0.75)\n",
    "        q25 = torch.quantile(S, 0.25)\n",
    "        iqr = q75 - q25\n",
    "        sigma_estimated = iqr / 1.349  # 0.6745 * sigma is the expected range between the quantiles (Q1 and Q3)\n",
    "        return sigma_estimated\n",
    "\n",
    "    def restore_model_original_layer(self, layer_type, layer_number):\n",
    "        layer_id = f\"{layer_type}_{layer_number}\"\n",
    "        for name, module in self.model.named_modules():\n",
    "            if layer_type in name and layer_number in name:\n",
    "                if name in self.original_weights:\n",
    "                    module.weight = torch.nn.Parameter(self.original_weights[name])\n",
    "                    print(f\"Restored original weights for layer: {name}\")\n",
    "                    if layer_id in self.modified_layers:\n",
    "                        self.modified_layers.remove(layer_id)\n",
    "                else:\n",
    "                    print(f\"No original weights saved for layer: {name}\")\n",
    "\n",
    "    # def calculate_model_perplexity(self, datasets=['wikitext2', 'c4', 'ptb'], seqlen=384, use_cuda_graph=False, use_flash_attn=False):\n",
    "    #     model = self.model\n",
    "    #     model_str = self.model_name\n",
    "    #     acc_loss = 0.0\n",
    "    #     total_samples = 0\n",
    "\n",
    "    #     for dataset in datasets:\n",
    "    #         input_tok = gptq_data_utils.get_test_tokens(dataset, seed=0, seqlen=seqlen, model=model_str)\n",
    "    #         nsamples = input_tok.numel() // seqlen\n",
    "    #         input_tok = input_tok[0, :(seqlen * nsamples)].view(nsamples, seqlen)\n",
    "    #         total_samples += nsamples\n",
    "\n",
    "    #         #if not use_cuda_graph:\n",
    "    #         #    model.reset()\n",
    "\n",
    "    #         loss_fct = torch.nn.CrossEntropyLoss().cuda()\n",
    "    #         progress = tqdm(range(nsamples))\n",
    "    #         for ii in progress:\n",
    "    #             input = input_tok[ii, :].cuda().view(1, -1)\n",
    "    #             output = model(input, use_cache=False, output_hidden_states=False, output_attentions=False)[0]\n",
    "    #             shift_logits = output[:, :-1, :].contiguous()\n",
    "    #             shift_labels = input[:, 1:]\n",
    "    #             loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    #             acc_loss += loss.item()\n",
    "    #             progress.set_description(f\"avg_loss = {acc_loss/(ii+1)}\")\n",
    "\n",
    "    #     avg_loss = acc_loss / total_samples\n",
    "    #     ppl = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    #     return ppl\n",
    "\n",
    "    # ### Implement a Backward Search\n",
    "    # # Search for the optimal lower ranking approximations from the top layers downwards\n",
    "    # # Also, we try doing a greedy approach, in order to maximize the rank reduction.\n",
    "    # # We tune the compression rate based on Marchenko-Pastur Random Matrix Theory\n",
    "    # ######################################################################################\n",
    "\n",
    "    # def search_optimal_layer_modification(self, layer_types, layer_numbers, max_mod=5):\n",
    "    #     # Calculate initial perplexity with original model weights\n",
    "    #     initial_perplexity = self.calculate_model_perplexity()\n",
    "    #     print(\"=\"*50)\n",
    "    #     print(f\"The initial perplexity of the model is {initial_perplexity}\")\n",
    "    #     print(\"=\"*50)\n",
    "    #     min_loss = initial_perplexity\n",
    "    #     optimal_params = (None, None)\n",
    "    #     mods = 0\n",
    "\n",
    "    #     for layer_number in layer_numbers:\n",
    "    #         for layer_type in layer_types:\n",
    "    #             if mods >= max_mod and max_mod != -1:\n",
    "    #                 return optimal_params, min_loss\n",
    "    #             attempt = (layer_type, layer_number)\n",
    "    #             if attempt in self.failed_attempts:\n",
    "    #                 continue  # Skip this attempt if it has failed before\n",
    "\n",
    "    #             try_update = self.update_model_reduce_layer(layer_type, layer_number)\n",
    "\n",
    "    #             if not try_update:\n",
    "    #                 continue  # Skip this attempt if it has already been modified before\n",
    "\n",
    "    #             try:\n",
    "    #                 loss = self.calculate_model_perplexity()\n",
    "    #                 if loss < min_loss:\n",
    "    #                     min_loss = loss\n",
    "    #                     optimal_params = (layer_type, layer_number)\n",
    "    #                     mods = mods + 1\n",
    "    #                     # Break out of the loop as soon as a better configuration is found\n",
    "    #                     print(\"*\"*50)\n",
    "    #                     print(f\"Improved perplexity found: {min_loss} for layer {layer_type} {layer_number}. Total modifications is {mods}\")\n",
    "    #                     print(\"*\"*50)\n",
    "    #                 else:\n",
    "    #                     self.restore_model_original_layer(layer_type, layer_number)\n",
    "    #                     self.failed_attempts.add(attempt)  # Record the failed attempt\n",
    "\n",
    "    #             except NotImplementedError:\n",
    "    #                 print(\"Perplexity calculation method is not implemented yet.\")\n",
    "    #                 return False, min_loss\n",
    "\n",
    "    #     return optimal_params, min_loss\n",
    "\n",
    "    def get_top_snr_ratios(self, top_n=16):\n",
    "        # Sort and extract the top n SNR values for each specific module\n",
    "        top_snr_layers = {}\n",
    "        for layer_type, snr_ratios in self.layer_snr.items():\n",
    "            sorted_layers = sorted(snr_ratios, key=lambda x: x[1], reverse=True)  # Sort by SNR value\n",
    "            top_snr_layers[layer_type] = [layer[0] for layer in sorted_layers[:top_n]]  # Saving the layer names\n",
    "\n",
    "        return top_snr_layers\n",
    "\n",
    "    def get_random_layers(self, layer_types, n=16):\n",
    "        random_layers = []\n",
    "\n",
    "        for layer_type in layer_types:\n",
    "            layers = []\n",
    "            for name, _ in self.model.named_modules():\n",
    "                if layer_type in name:\n",
    "                    layers.append(name)\n",
    "            shuffle(layers)\n",
    "            random_layers += layers[:n]\n",
    "\n",
    "        return random_layers\n",
    "\n",
    "    def save_layers_to_json(self, filename=\"zzz_rmt_laser/layer_snr_info.json\"):\n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(self.layer_snr, file, indent=4)\n",
    "\n",
    "    def save_top_snr_ratios_to_json(self, top_snr_layers, filename=\"zzz_rmt_laser/top_snr_ratios.json\"):\n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(top_snr_layers, file, indent=4)\n",
    "\n",
    "    def save_top_snr_ratios_to_txt(self, top_snr_layers, filename=\"zzz_rmt_laser/top_snr_ratios.txt\"):\n",
    "        names = []\n",
    "        for layers in top_snr_layers.values():\n",
    "            names += layers\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.writelines([name + \"\\n\" for name in names])\n",
    "\n",
    "    def save_model(self, save_dir):\n",
    "        self.model.save_pretrained(save_dir)\n",
    "        self.tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbdbc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only setup for nllb\n",
    "def create_tied_embedding_weights(model: PreTrainedModel) -> PreTrainedModel:\n",
    "    encoder_embeddings = torch.nn.Embedding(model.config.vocab_size, model.config.d_model, model.config.pad_token_id)\n",
    "    decoder_embeddings = torch.nn.Embedding(model.config.vocab_size, model.config.d_model, model.config.pad_token_id)\n",
    "    model.base_model.encoder.embed_tokens = encoder_embeddings\n",
    "    model.base_model.decoder.embed_tokens = decoder_embeddings\n",
    "    model.tie_weights()\n",
    "    return model\n",
    "\n",
    "\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "layer_numbers = [f\".{layer}.\" for layer in range(11, -1, -1)]\n",
    "layer_types = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"]\n",
    "\n",
    "modifier = ModelModifier(model_name)\n",
    "\n",
    "# NOTE: not sure what this code was for\n",
    "# # load normal fine tuned model\n",
    "# modifier.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#     \"test_S/MT/experiments/NLLB.1.3B.id-XriAlasSplit_8001.btx-XriAlasSplit_8001/run/checkpoint-2000\",\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "# modifier.tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"test_S/MT/experiments/NLLB.1.3B.id-XriAlasSplit_8001.btx-XriAlasSplit_8001\"\n",
    "# )\n",
    "\n",
    "# # load and merge lora model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"test_S/MT/experiments/lora_32_all\", use_fast=True)\n",
    "# base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "# base_model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\n",
    "# base_model = create_tied_embedding_weights(base_model)\n",
    "# model = PeftModel.from_pretrained(base_model, \"test_S/MT/experiments/lora_32_all/run/checkpoint-14000\")\n",
    "# model.merge_and_unload()\n",
    "# modifier.model = model\n",
    "# modifier.tokenizer = tokenizer\n",
    "\n",
    "# normal process\n",
    "modifier.assess_layers_snr(layer_types, layer_numbers)\n",
    "top_snr_ratios = modifier.get_top_snr_ratios(16)\n",
    "\n",
    "modifier.save_layers_to_json()\n",
    "modifier.save_top_snr_ratios_to_json(top_snr_ratios)\n",
    "modifier.save_top_snr_ratios_to_txt(top_snr_ratios)\n",
    "\n",
    "# # random layers\n",
    "# n = 4\n",
    "# random_layers = modifier.get_random_layers(layer_types, n)\n",
    "# with open(f\"random_layers_{n}_nllb.txt\", \"w\") as f:\n",
    "#     f.writelines([name + \"\\n\" for name in random_layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e152492",
   "metadata": {},
   "source": [
    "### Code snippets that I inserted into HuggingFaceNMTModel.train in order to run the experiments\n",
    "In my experiment directories, I had a `target_layers.txt` file that was the output of the `save_top_snr_ratios_to_txt` function above, which is just a list of layer names. The candidate layers are all of the \"Linear\" layers of the model. In (I believe) all of the experiments, the embedding layers were fully trained (unfrozen, no reduced rank)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e57c9f",
   "metadata": {},
   "source": [
    "**Targeting layers**\n",
    "\n",
    "This method was used in combination with LoRA. The target modules were trained with reduced rank and the embeddings were trained with full rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target layers\n",
    "target_modules = []\n",
    "target_layers_path = self._config.exp_dir / \"target_layers.txt\"\n",
    "with target_layers_path.open() as f:\n",
    "    target_modules = [line[:-1] for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9097fa72",
   "metadata": {},
   "source": [
    "**Freezing layers**\n",
    "\n",
    "This method just uses basic model functionality to freeze the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b647bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze layers\n",
    "# example layer name: model.encoder.layers.0.self_attn.q_proj\n",
    "modules_to_train = [\"model.shared\"] # embedding layers\n",
    "target_layers_path = self._config.exp_dir / \"target_layers.txt\"\n",
    "with target_layers_path.open() as f:\n",
    "    modules_to_train += [line[:-1] for line in f.readlines()]\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    freeze = True\n",
    "    for module in modules_to_train:\n",
    "        if name.startswith(module):\n",
    "            freeze = False\n",
    "    param.requires_grad = not freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba4df4",
   "metadata": {},
   "source": [
    "**Model reduction**\n",
    "\n",
    "This is the method proposed in the laserRMT paper/code. From what I remember, this is very CPU-instensive and ran extremely slow on the GPUs. I would recommend creating and saving the desired reduced model with a CPU (since the base model is what's being reduced) before fine-tuning it on a GPU. There is a model with all linear layers reduced located at `M/MT/experiments/Demo_Isaac/nllb_full_reduced` that was used for the 'full_reduced' experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f11240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_reduce_layer(model: PreTrainedModel, layer_name: str) -> PreTrainedModel:\n",
    "    module = model.get_submodule(layer_name)\n",
    "    original_dtype = module.weight.dtype\n",
    "    weights = module.weight.double()\n",
    "    U, S, V = torch.linalg.svd(weights, full_matrices=False)\n",
    "\n",
    "    # Estimate sigma using the full IQR method\n",
    "    q75 = torch.quantile(S, 0.75)\n",
    "    q25 = torch.quantile(S, 0.25)\n",
    "    iqr = q75 - q25\n",
    "    sigma_estimated = iqr / 1.349\n",
    "\n",
    "    # Calculate Marchenko-Pastur threshold\n",
    "    n, m = weights.shape\n",
    "    beta = n / m if n < m else m / n\n",
    "    mp_threshold_full_iqr = sigma_estimated * np.sqrt((1 + np.sqrt(beta)) ** 2)\n",
    "\n",
    "    # Retain only the singular values above the MP threshold\n",
    "    S_reduced = torch.zeros_like(S)\n",
    "    k = (S > mp_threshold_full_iqr).sum().item()\n",
    "    S_reduced[:k] = S[:k]\n",
    "    print(f\"Reduced from {S.shape} to {k}\")\n",
    "\n",
    "    # Reconstruct the matrix using the thresholded singular values\n",
    "    reconstructed_weights = U @ torch.diag(S_reduced) @ V\n",
    "    reconstructed_weights = reconstructed_weights.to(original_dtype)\n",
    "    module.weight = torch.nn.Parameter(reconstructed_weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "# reduce model\n",
    "modules_to_train = []\n",
    "target_layers_path = self.config.exp_dir / \"target_layers.txt\"\n",
    "with target_layers_path.open() as f:\n",
    "    modules_to_train += [line[:-1] for line in f.readlines()]\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(self.config.model, device_map=\"auto\")\n",
    "for layer in modules_to_train:\n",
    "    model = update_model_reduce_layer(model, layer)\n",
    "model.save_pretrained(self.config.exp_dir / \"reduced\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "silnlp-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
