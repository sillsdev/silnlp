{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment w/ more aligning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from silnlp.common.corpus import load_corpus, write_corpus\n",
    "from silnlp.alignment.config import get_aligner\n",
    "from silnlp.alignment.machine_aligner import MachineAligner\n",
    "from typing import List, Union\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from machine.tokenization import LatinWordTokenizer\n",
    "from machine.corpora import escape_spaces, nfc_normalize, lowercase\n",
    "\n",
    "# Tokenize and normalize sentences in the same way as the normal alignment process\n",
    "def tokenize_for_alignment(sents: List[Union[str, List[str]]]) -> List[str]:\n",
    "    if type(sents[0]) == str:\n",
    "        tokenizer = LatinWordTokenizer()\n",
    "        sents = [tokenizer.tokenize(sent) for sent in sents]\n",
    "    sents_norm = [lowercase(nfc_sent) for nfc_sent in [nfc_normalize(es_sent) for es_sent in [escape_spaces(sent) for sent in sents]]]\n",
    "    return [\" \".join(toks) for toks in sents_norm]\n",
    "\n",
    "def align_sents(src_sents: List[str],\n",
    "                trg_sents: List[str],\n",
    "                aligner_id: str = \"hmm\",\n",
    "                sym_align_path: Path = None,\n",
    "                extra_train_data_src: Path = None,\n",
    "                extra_train_data_trg: Path = None\n",
    "                ) -> None:\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        temp_dir = Path(td)\n",
    "\n",
    "        # Since the sentences get tokenized before they are normalized in the normal alignment process,\n",
    "        # we can wait to do the normalization until here\n",
    "        src_sents_norm = tokenize_for_alignment(src_sents)\n",
    "        trg_sents_norm = tokenize_for_alignment(trg_sents)\n",
    "\n",
    "        # Prep alignment data\n",
    "        align_src_path = temp_dir / \"align.src.txt\"\n",
    "        align_trg_path = temp_dir / \"align.trg.txt\"\n",
    "        write_corpus(align_src_path, src_sents_norm)\n",
    "        write_corpus(align_trg_path, trg_sents_norm)\n",
    "\n",
    "        # Prep training data\n",
    "        if extra_train_data_src and extra_train_data_trg:\n",
    "            src_sents_norm += tokenize_for_alignment(list(load_corpus(extra_train_data_src)))\n",
    "            trg_sents_norm += tokenize_for_alignment(list(load_corpus(extra_train_data_trg)))\n",
    "        train_src_path = temp_dir / \"train.src.txt\"\n",
    "        train_trg_path = temp_dir / \"train.trg.txt\"\n",
    "        write_corpus(train_src_path, src_sents_norm)\n",
    "        write_corpus(train_trg_path, trg_sents_norm)\n",
    "\n",
    "        # Train the aligner and align\n",
    "        aligner: MachineAligner = get_aligner(aligner_id, temp_dir)\n",
    "\n",
    "        aligner.train(train_src_path, train_trg_path)\n",
    "        aligner.force_align(align_src_path, align_trg_path, sym_align_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USFM marker preservation\n",
    "* Extract footnotes and put them at the end\n",
    "* Extract each instance of a marker and record its index\n",
    "* Tokenize source sentences and match each marker to surrounding tokens based on their original indices\n",
    "* Train aligner on all training data + translation, align translation to source\n",
    "* Reinsert marker instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "'''Define project values'''\n",
    "pair = \"\"\n",
    "project = \"\"\n",
    "file_suffix = \"\"\n",
    "trg_project = \"\"\n",
    "trg_file_suffix = \"\"\n",
    "\n",
    "book = \"MAT\"\n",
    "book_name = f\"41{book}\"\n",
    "draft = True\n",
    "\n",
    "src_fpath = Path(f\"test_S/Paratext/projects/{project}/{book_name}{file_suffix}.SFM\")\n",
    "if draft:\n",
    "    trg_fpath = Path(f\"zzz_USFM/{pair}/{book}/{book_name}_draft.SFM\")\n",
    "else:\n",
    "    trg_fpath = Path(f\"test_S/Paratext/projects/{trg_project}/{book_name}{trg_file_suffix}.SFM\")\n",
    "aligner = \"eflomal\"\n",
    "pair_book_dir = Path(f\"zzz_USFM/{pair}/{book}\")\n",
    "align_path = pair_book_dir / f\"{book_name}_sym-align_{aligner}.txt\"\n",
    "out_fpath = pair_book_dir / f\"{book_name}{trg_file_suffix}_out.SFM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.corpora import FileParatextProjectSettingsParser, UsfmFileText, UsfmTokenizer\n",
    "from machine.tokenization import LatinWordTokenizer\n",
    "\n",
    "src_settings = FileParatextProjectSettingsParser(src_fpath.parent).parse()\n",
    "src_file_text = UsfmFileText(\n",
    "    src_settings.stylesheet,\n",
    "    src_settings.encoding,\n",
    "    book,\n",
    "    src_fpath,\n",
    "    src_settings.versification,\n",
    "    include_markers=True, # F/T gives notes their own rows, F/F gives just the main text, T/F gives one ref per verse and all markers are inline\n",
    "    include_all_text=True, # T/T includes all intro and section titles (as does F/T), all other notes/markers inline\n",
    "    project=src_settings.name,\n",
    ")\n",
    "\n",
    "sentences = []\n",
    "vrefs = []\n",
    "for sent in src_file_text:\n",
    "    if (len(sent.ref.path) == 0 or sent.ref.path[-1].name != \"rem\") and len(sent.text.strip()) > 0:\n",
    "        sentences.append(sent.text.strip())\n",
    "        vrefs.append(sent.ref)\n",
    "        print(vrefs[-1], sentences[-1])\n",
    "\n",
    "# for ref, sent in zip(vrefs, sentences):\n",
    "#     print(ref, sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only deal with paragraph markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.corpora import UsfmTokenizer, UsfmTokenType, UsfmStyleType\n",
    "# TODO: would it be easier to always use StyleType (for UsfmTags) vs TokenType (for UsfmTokens)?\n",
    "\n",
    "'''Parse sentences'''\n",
    "tokenizer = UsfmTokenizer(src_settings.stylesheet)\n",
    "sentence_toks = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "to_delete = [\"fig\"]\n",
    "inline_markers = []\n",
    "# markers_by_verse = [[] for _ in sentence_toks]\n",
    "text_only_sents = [\"\" for _ in sentence_toks]\n",
    "ignored_segments = []\n",
    "for i, (toks, ref) in enumerate(zip(sentence_toks, vrefs)):\n",
    "    ignored_segment = \"\"\n",
    "    ignore_scope = None\n",
    "    for j, tok in enumerate(toks): # POSSIBLE TYPES: TEXT, PARAGRAPH, CHARACTER, NOTE, END\n",
    "        if ignore_scope is not None:\n",
    "            ignored_segment += tok.to_usfm()\n",
    "            if tok.type == UsfmTokenType.END and tok.marker[:-1] == ignore_scope.marker:\n",
    "                ignored_segments.append((ref, ignored_segment))\n",
    "                ignored_segment = \"\"\n",
    "                ignore_scope = None\n",
    "        elif tok.type == UsfmTokenType.NOTE or (tok.type == UsfmTokenType.CHARACTER and tok.marker in to_delete):\n",
    "            ignore_scope = tok\n",
    "            ignored_segment += tok.to_usfm()\n",
    "        elif tok.type in [UsfmTokenType.PARAGRAPH, UsfmTokenType.CHARACTER, UsfmTokenType.END]: # possible tok.marker forms: w w* +w +w*\n",
    "            # len of text so far == start idx of next text tok, this won't break with other markers bc the idx is wrt only the text toks \n",
    "            inline_markers.append((i, len(text_only_sents[i]), tok.to_usfm())) # previously tok.to_usfm() so it could be inserted into the text\n",
    "        elif tok.type == UsfmTokenType.TEXT:\n",
    "            text_only_sents[i] += tok.text\n",
    "        # elif tok.type != UsfmTokenType.CHARACTER and tok.type != UsfmTokenType.END:\n",
    "        #     print(tok.type, tok)\n",
    "\n",
    "# print(\"sent_idx, orig_idx, marker\")\n",
    "# for marker in inline_markers:\n",
    "#     print(marker)\n",
    "# for ref, sent in zip(vrefs, text_only_sents):\n",
    "#     print(ref, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.corpora import FileParatextProjectSettingsParser, UsfmFileText\n",
    "from pathlib import Path\n",
    "\n",
    "'''Translate sentences (or use target sentences for testing)'''\n",
    "\n",
    "# Get target file and remove all markers\n",
    "if draft:\n",
    "    trg_file_text = UsfmFileText(\n",
    "        src_settings.stylesheet,\n",
    "        src_settings.encoding,\n",
    "        book,\n",
    "        trg_fpath,\n",
    "        src_settings.versification,\n",
    "        include_markers=True,\n",
    "        include_all_text=True,\n",
    "        project=src_settings.name,\n",
    "    )\n",
    "else:\n",
    "    trg_settings = FileParatextProjectSettingsParser(trg_fpath.parent).parse()\n",
    "    trg_file_text = UsfmFileText(\n",
    "        trg_settings.stylesheet,\n",
    "        trg_settings.encoding,\n",
    "        trg_settings.get_book_id(trg_fpath.name),\n",
    "        trg_fpath,\n",
    "        trg_settings.versification,\n",
    "        include_markers=True,\n",
    "        include_all_text=True,\n",
    "        project=trg_settings.name,\n",
    "    )\n",
    "    tokenizer = UsfmTokenizer(trg_settings.stylesheet)\n",
    "\n",
    "trg_sents = []\n",
    "trg_vrefs = []\n",
    "for sent in trg_file_text:\n",
    "    if (len(sent.ref.path) > 0 and sent.ref.path[-1].name == \"rem\") or len(sent.text.strip()) == 0:\n",
    "        continue\n",
    "    trg_sents.append(\"\")\n",
    "    trg_vrefs.append(sent.ref)\n",
    "\n",
    "    sent = sent.text.strip()\n",
    "    usfm_toks = tokenizer.tokenize(sent)\n",
    "    ignore_scope = None\n",
    "    for j, tok in enumerate(usfm_toks): # POSSIBLE TYPES: TEXT, PARAGRAPH, CHARACTER, NOTE, END\n",
    "        if ignore_scope is not None:\n",
    "            if tok.type == UsfmTokenType.END and tok.marker[:-1] == ignore_scope.marker:\n",
    "                ignore_scope = None\n",
    "        elif tok.type == UsfmTokenType.NOTE or (tok.type == UsfmTokenType.CHARACTER and tok.marker in to_delete):\n",
    "            ignore_scope = tok\n",
    "        elif tok.type == UsfmTokenType.TEXT:\n",
    "            trg_sents[-1] += tok.text\n",
    "\n",
    "# for ref, sent in zip(trg_vrefs, trg_sents):\n",
    "#     print(ref, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "'''Tokenize sentences'''\n",
    "tokenizer = LatinWordTokenizer()\n",
    "\n",
    "# TODO: force ranges to not cross boundaries by tokenizing each text token separately? or bound the alignment range some other way based on surrounding markers\n",
    "# is there a simpler way to force markers to be reinserted in the same relative order?\n",
    "word_tok_ranges = [list(tokenizer.tokenize_as_ranges(sent)) for sent in text_only_sents]\n",
    "toks = [[sent[r.start : r.end] for r in ranges] for sent, ranges in zip(text_only_sents, word_tok_ranges)]\n",
    "\n",
    "trg_word_tok_ranges = [list(tokenizer.tokenize_as_ranges(sent)) for sent in trg_sents]\n",
    "trg_toks = [[sent[r.start : r.end] for r in ranges] for sent, ranges in zip(trg_sents, trg_word_tok_ranges)]\n",
    "\n",
    "'''Match markers to the token closest to them'''\n",
    "# TODO: need to disambiguate the order of markers in the case where there are multiple markers in a row\n",
    "# Returns a list of the indices of the the tokens following each of the input sequences\n",
    "# The returned indices are based on the tokens post-marker removal, but the lookup is based on the character indices of the original strings\n",
    "def get_toks_after_sequences(sequences: List[Tuple]) -> List[int]:\n",
    "    toks_after_seqs = []\n",
    "    for sequence in sequences:\n",
    "        sent_idx, start_idx = sequence[0], sequence[1] # now, start_idx is wrt text_only_sents\n",
    "        for i, tok_range in reversed(list(enumerate(word_tok_ranges[sent_idx]))):\n",
    "            # this works fine but there's still risk of splitting words if the token goes across paragraph boundaries\n",
    "            # this can be fixed by forcing tok boundaries to stay within usfm text toks\n",
    "            if tok_range.start < start_idx:\n",
    "                toks_after_seqs.append(i+1)\n",
    "                break\n",
    "            if i == 0:\n",
    "                toks_after_seqs.append(i)\n",
    "\n",
    "    return toks_after_seqs\n",
    "toks_after_markers = get_toks_after_sequences(inline_markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from silnlp.alignment.utils import compute_alignment_scores\n",
    "from silnlp.common.corpus import write_corpus\n",
    "'''Align sentences'''\n",
    "# align_sents(toks, trg_toks, aligner, align_path) # Path(f\"zzz_PN_KTs/tpi_aps/train.src.detok.txt\"),Path(f\"zzz_PN_KTs/tpi_aps/train.trg.detok.txt\")\n",
    "\n",
    "# eflomal\n",
    "if not align_path.exists():\n",
    "    write_corpus(pair_book_dir / \"src_align.txt\", text_only_sents) # orig sentences\n",
    "    write_corpus(pair_book_dir / \"trg_align.txt\", trg_sents)\n",
    "    compute_alignment_scores(pair_book_dir / \"src_align.txt\", pair_book_dir / \"trg_align.txt\", aligner, align_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from silnlp.common.corpus import load_corpus\n",
    "\n",
    "# probably non-exhaustive, second of same-looking single closing quotes is an apostrophe, I think\n",
    "# QUOTATION_MARKS = [\"'\", '\"', \"“\", \"”\", \"‘\", \"’\", \"ʼ\", \"<\", \">\"]\n",
    "\n",
    "'''Decide where to reinsert markers'''\n",
    "\n",
    "# read aligner output\n",
    "if aligner == \"eflomal\":\n",
    "    align_lines = [[(lambda x: (int(x[0]), int(x[1])))(pair.split(\"-\")) for pair in line.split()] for line in load_corpus(align_path)]\n",
    "else:\n",
    "    align_lines = [[(lambda x: (int(x[0]), int(x[1])))(pair.split(\":\")[0].split(\"-\")) for pair in line.split()] for line in load_corpus(align_path)]\n",
    "\n",
    "# Gets the number of alignment pairs that \"cross the line\" between (src_idx - .5) and (trg_idx - .5)\n",
    "def num_align_crossings(sent_idx: int, src_idx: int, trg_idx: int) -> int:\n",
    "    crossings = 0\n",
    "    for i,j in align_lines[sent_idx]:\n",
    "        if i < src_idx and j >= trg_idx:\n",
    "            crossings += 1\n",
    "        if i >= src_idx and j < trg_idx:\n",
    "            crossings += 1\n",
    "    return crossings\n",
    "\n",
    "# Get the index of the trg token that each sequence should be inserted before\n",
    "def get_insert_indices(seqs_to_insert: List[Tuple], adj_tok_idxs: List[int]) -> List[int]:\n",
    "    punct_hyp_freqs = defaultdict(lambda: defaultdict(int))\n",
    "    hyp_freqs = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    insert_indices = []\n",
    "    for seq, adj_src_tok in zip(seqs_to_insert, adj_tok_idxs):\n",
    "        sent_idx = seq[0]\n",
    "        marker = seq[2].strip(\" \\\\+\")\n",
    "\n",
    "        '''Original, checks first hypothesis of its original position and one tok previous and picks the better of the two'''\n",
    "        # If the token on either side of a hypothesis is punctuation, use that\n",
    "        # can also try a less extreme version where the punct hyps are still subject to having the least crossings, but they are still always checked first\n",
    "\n",
    "        trg_hyp = -1\n",
    "        punct_hyps = [-1, 0]\n",
    "        for punct_hyp in punct_hyps:\n",
    "            src_hyp = adj_src_tok + punct_hyp\n",
    "            if src_hyp < 0 or src_hyp >= len(toks[sent_idx]):\n",
    "                continue\n",
    "            # only accept pairs where both the src and trg token are punct\n",
    "            # can define more specifically what the punct tokens can look like later\n",
    "            if len(toks[sent_idx][src_hyp]) > 0 and not any(c.isalpha() for c in toks[sent_idx][src_hyp]):\n",
    "                align_pairs = reversed(align_lines[sent_idx]) if punct_hyp < 0 else align_lines[sent_idx]\n",
    "                for s,t in align_pairs:\n",
    "                    if s == src_hyp and not any(c.isalpha() for c in trg_toks[sent_idx][t]):\n",
    "                        trg_hyp = t\n",
    "                        break\n",
    "            if trg_hyp != -1:\n",
    "                # if this search gets expanded beyond [-1,0] can do insert_idx -= punct_hyp\n",
    "                insert_idx = trg_hyp + 1 if punct_hyp < 0 else trg_hyp\n",
    "                insert_indices.append(insert_idx)\n",
    "                punct_hyp_freqs[marker][punct_hyp] += 1\n",
    "                break\n",
    "        if trg_hyp != -1:\n",
    "            continue\n",
    "\n",
    "        hyps = [0, 1, 2]\n",
    "        best_hyp = (-1, None, len(align_lines[sent_idx]))\n",
    "        checked = set() # to prevent checking the same idx twice\n",
    "        for hyp in hyps:\n",
    "            align_pairs = reversed(align_lines[sent_idx]) if hyp < 0 else align_lines[sent_idx]\n",
    "            src_hyp = adj_src_tok + hyp\n",
    "            if src_hyp in checked:\n",
    "                continue\n",
    "            trg_hyp = -1\n",
    "            while trg_hyp == -1 and src_hyp >= 0 and src_hyp < len(toks[sent_idx]):\n",
    "                checked.add(src_hyp)\n",
    "                trg_hyps = [t for (s,t) in align_pairs if s == src_hyp]\n",
    "                if len(trg_hyps) > 0:\n",
    "                    trg_hyp = trg_hyps[0]\n",
    "                else:\n",
    "                    src_hyp += -1 if hyp < 0 else 1\n",
    "            if trg_hyp != -1:\n",
    "                num_crossings = num_align_crossings(sent_idx, src_hyp, trg_hyp)\n",
    "                if num_crossings < best_hyp[2]:\n",
    "                    # replace hyp with src_hyp - adj_src_tok\n",
    "                    # above is what I was trying to do before with offsetting the farther away hyps, but it's worse\n",
    "                    # am I doing this below now with \"insert_idx = insert_idx - best_hyp[1]\"?\n",
    "                    best_hyp = (trg_hyp, hyp, num_crossings)\n",
    "\n",
    "        if best_hyp[0] == -1:\n",
    "            insert_indices.append(len(trg_toks[sent_idx])) # insert at the end of the sentence\n",
    "            hyp_freqs[marker][\"none\"] += 1\n",
    "        else:\n",
    "            # insert_idx = best_hyp[0]\n",
    "            # # do this before or after subtracting the offset? only subtract offset if not adjacent to punct? check before and after subtraction?\n",
    "            # if trg_toks[sent_idx][insert_idx] in [\",\", \".\", \"!\", \"?\"]:\n",
    "            #     insert_idx += 1\n",
    "            # # subtracting hyp at the end may be bad in other cases, or make wrong answers worse/more confusing\n",
    "            # else:\n",
    "\n",
    "            # insert_idx = insert_idx - best_hyp[1]\n",
    "            # if trg_toks[sent_idx][insert_idx] in [\",\", \".\", \"!\", \"?\"]:\n",
    "            #     insert_idx += 1\n",
    "\n",
    "            insert_indices.append(best_hyp[0])\n",
    "            # for end markers, preventing double adding (from punct first) was almost the same but slightly worse\n",
    "            hyp_freqs[marker][best_hyp[1]] += 1\n",
    "\n",
    "    print({k: dict(v) for k,v in punct_hyp_freqs.items()})\n",
    "    print({k: dict(v) for k,v in hyp_freqs.items()})\n",
    "    \n",
    "    return insert_indices\n",
    "\n",
    "trg_toks_after_markers = get_insert_indices(inline_markers, toks_after_markers)\n",
    "\n",
    "print(toks_after_markers)\n",
    "print(trg_toks_after_markers)\n",
    "print(len(trg_toks_after_markers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: right now, half the word for the insertion order is done when to_insert is filled out, and the other half is done when the markers are\n",
    "being inserted (with reverse). Since there there's already funky stuff going on in the order of the markers in to_insert (for disambiguation \n",
    "for the same insertion idx), it would make more sense to just do all the ordering when to_insert is being filled out, i.e. the order of a list\n",
    "in to_insert is the order they need to be inserted in\n",
    "on the other hand, the current way might be more human-readable\n",
    "'''\n",
    "\n",
    "'''Reinsert markers'''\n",
    "to_insert = [[] for _ in vrefs]\n",
    "\n",
    "# Collect the markers to be inserted\n",
    "for i, (mark, next_trg_tok) in enumerate(zip(inline_markers, trg_toks_after_markers)):\n",
    "    sent_idx, _, marker = mark\n",
    "    if next_trg_tok >= len(trg_word_tok_ranges[sent_idx]): # TODO: this shouldn't happen\n",
    "        insert_idx = len(trg_sents[sent_idx])\n",
    "    else:\n",
    "        insert_idx = trg_word_tok_ranges[sent_idx][next_trg_tok].start\n",
    "\n",
    "    # figure out the order of the markers in the sentence to handle ambiguity for directly adjacent markers\n",
    "    insert_place = 0\n",
    "    while insert_place < len(to_insert[sent_idx]) and to_insert[sent_idx][insert_place][0] <= insert_idx:\n",
    "        insert_place += 1\n",
    "\n",
    "    to_insert[sent_idx].insert(insert_place, (insert_idx, marker))\n",
    "\n",
    "'''create rows for each paragraph marker and insert character markers back into text'''\n",
    "# Construct rows to update the USFM file with\n",
    "rows = []\n",
    "for sent_idx, (ref, trg_sent) in enumerate(zip(vrefs, trg_sents)):\n",
    "    row_texts = []\n",
    "    attach_to_prev = False # TODO: better name\n",
    "    prev_is_end = False # hacky\n",
    "    for insert_idx, marker in reversed(to_insert[sent_idx]):\n",
    "        is_char_marker = src_settings.stylesheet.get_tag(marker.strip(\" \\\\+*\")).style_type == UsfmStyleType.CHARACTER\n",
    "        row_text = (marker if is_char_marker else \"\") \\\n",
    "                    + (\" \" if \"*\" in marker and insert_idx < len(trg_sent) and trg_sent[insert_idx].isalpha() else \"\") \\\n",
    "                    + trg_sent[insert_idx:]\n",
    "\n",
    "        if attach_to_prev:\n",
    "            # don't want a space before end marker\n",
    "            if prev_is_end and len(row_text) > 0 and row_text[-1] == \" \": # hacky\n",
    "                row_text = row_text[:-1]\n",
    "            # append text segments instead of creating a new one since previous segment (really the next segment bc iterating backwards) is not its own paragraph\n",
    "            row_texts[0] = row_text + row_texts[0]\n",
    "        else:\n",
    "            row_texts.insert(0, row_text)\n",
    "\n",
    "        # only paragraph markers get their own rows, so all segments split by character markers and their end markers need to be rejoined\n",
    "        attach_to_prev = is_char_marker\n",
    "        prev_is_end = \"*\" in marker\n",
    "        trg_sent = trg_sent[:insert_idx]\n",
    "\n",
    "    # do the same as above with the text at the beginning of the verse\n",
    "    if attach_to_prev:\n",
    "        if prev_is_end and len(trg_sent) > 0 and trg_sent[-1] == \" \": # hacky\n",
    "            trg_sent = trg_sent[:-1]\n",
    "        row_texts[0] = trg_sent + row_texts[0]\n",
    "    else:\n",
    "        rows.append(([ref], trg_sent))\n",
    "\n",
    "    for row_text in row_texts:\n",
    "        rows.append(([ref], row_text))\n",
    "\n",
    "# add footnotes to ends of versess\n",
    "for i, (ref, sent) in enumerate(rows):\n",
    "    ref = ref[0]\n",
    "    if i < len(rows) - 1 and rows[i+1][0][0].verse_ref == ref.verse_ref:\n",
    "        continue\n",
    "    while len(ignored_segments) > 0 and ignored_segments[0][0].verse_ref == ref.verse_ref:\n",
    "        rows[i] = ([ref], rows[i][1] + ignored_segments[0][1])\n",
    "        ignored_segments.pop(0)\n",
    "\n",
    "print(len(rows))\n",
    "# for ref, sent in rows:\n",
    "#     print(ref, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.corpora import UpdateUsfmParserHandler, parse_usfm, UsfmParserState, UpdateUsfmBehavior\n",
    "\n",
    "class ParagraphUpdateUsfmParserHandler(UpdateUsfmParserHandler):\n",
    "    def _collect_tokens(self, state: UsfmParserState) -> None:\n",
    "        self._tokens.extend(self._new_tokens)\n",
    "        self._new_tokens.clear()\n",
    "        while self._token_index <= state.index + state.special_token_count:\n",
    "            if state.tokens[self._token_index].type == UsfmTokenType.PARAGRAPH and state.tokens[self._token_index].marker != \"rem\":\n",
    "                num_text = 0\n",
    "                rem_offset = 0\n",
    "                for i in range(len(self._tokens) - 1, -1, -1):\n",
    "                    if self._tokens[i].type == UsfmTokenType.TEXT:\n",
    "                        num_text += 1\n",
    "                    elif self._tokens[i].type == UsfmTokenType.PARAGRAPH and self._tokens[i].marker == \"rem\":\n",
    "                        rem_offset += num_text + 1\n",
    "                        num_text = 0\n",
    "                    else:\n",
    "                        break\n",
    "                if num_text >= 2:\n",
    "                    self._tokens.insert(-(rem_offset + num_text - 1), state.tokens[self._token_index])\n",
    "                    self._token_index += 1\n",
    "                    break # should this be continue instead? is there just no difference bc only 1 paragraph marker is added at a time?\n",
    "            self._tokens.append(state.tokens[self._token_index])\n",
    "            self._token_index += 1\n",
    "\n",
    "'''Update USFM and write out'''\n",
    "# preserve_whitespace=True doesn't change anything with markers on newlines but it does take care of the \\vp\\*vp somehow\n",
    "with open(src_fpath, encoding=src_settings.encoding) as f:\n",
    "    usfm = f.read()\n",
    "handler = ParagraphUpdateUsfmParserHandler(rows, behavior=UpdateUsfmBehavior.PREFER_NEW)\n",
    "parse_usfm(usfm, handler, src_settings.stylesheet, src_settings.versification, preserve_whitespace=False)\n",
    "usfm_out = handler.get_usfm(src_settings.stylesheet)\n",
    "\n",
    "with out_fpath.open(\"w\", encoding=src_settings.encoding) as f:\n",
    "    f.write(usfm_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
